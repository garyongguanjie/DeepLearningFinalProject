{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import pickle\n",
    "from data_loader import get_loader,CocoDataset\n",
    "from build_vocab import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.VOCAB_PATH, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "TRAIN_LOADER = {'root':config.TRAIN_FEATURE_PATH, 'json':config.TRAIN_JSON_PATH, 'vocab':vocab, 'batch_size':32, 'shuffle':True, 'num_workers':4}\n",
    "VAL_LOADER = {'root':config.VAL_FEATURE_PATH, 'json':config.VAL_JSON_PATH, 'vocab':vocab, 'batch_size':32, 'shuffle':False, 'num_workers':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.65s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_loader(**TRAIN_LOADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4530"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 49, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[32, 49, 1]}, size=[32, 49]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-b228c04d16f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mencoded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#     print(images.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch/DeepLearningFinalProject/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, captions, lengths)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0malphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[32, 49, 1]}, size=[32, 49]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "from model import EncoderCNN,DecoderRNN\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "encoder = EncoderCNN(512)\n",
    "decoder = DecoderRNN(256,300,vocab_size)\n",
    "for data in train_loader:\n",
    "    images,captions,lengths = data\n",
    "    images = images.permute(0,2,1)\n",
    "    encoded_images = encoder(images)\n",
    "    decoder(encoded_images,captions,lengths)\n",
    "    \n",
    "#     print(images.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderCNN(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,    4,   30,  110,    7,    4,  203,  584, 2395, 2176, 2404,   36,\n",
      "           39,   33,  219,   34,   19,    2],\n",
      "        [   1,    4,  349,  125,  918,  170, 2347,  552,   97,    4,   25,    7,\n",
      "          472,    4,  473,   19,    2,    0],\n",
      "        [   1,    4,  349,   75,    4,  899,  946,  215,   39,    4,  268,   75,\n",
      "           33,  269,   19,    2,    0,    0],\n",
      "        [   1,    4,  119,  311, 1419,   75, 1424,   46,  535,    7,    4,   20,\n",
      "          311,  312,   19,    2,    0,    0],\n",
      "        [   1,    4,  307,  662,  221,   22,  476,  478,   22,    4,   91,  331,\n",
      "           75, 1190,   19,    2,    0,    0],\n",
      "        [   1,    4,  319,   14, 1188,   22,    4,  163,  164,   39,  248,   14,\n",
      "          679,   19,    2,    0,    0,    0],\n",
      "        [   1,   33,   11,  125,   21,   22,    4,   20,  148,   39,    4,  891,\n",
      "           65,   19,    2,    0,    0,    0],\n",
      "        [   1,    4,  163, 1059,   33,    3,   14,   46,  312,  545,   39,   33,\n",
      "          188,   19,    2,    0,    0,    0],\n",
      "        [   1,  139,  125,  715,    4,  698, 1197,  199, 3256,   75,   33,  269,\n",
      "           19,    2,    0,    0,    0,    0],\n",
      "        [   1,  352,  103,  263,  350,  155,    4, 1168,  578,  543,  392, 1950,\n",
      "           19,    2,    0,    0,    0,    0],\n",
      "        [   1,    4,  305,   14, 1242,  350,  155,    4,  319,   14,  138, 1637,\n",
      "           19,    2,    0,    0,    0,    0],\n",
      "        [   1,  241,  125,    4, 1221,   22,    4,  202,   14, 4054,   39,   33,\n",
      "          119,    2,    0,    0,    0,    0],\n",
      "        [   1,    4, 1302, 1175,  239,   14,   33,  248,   14,    4,  119,  150,\n",
      "           19,    2,    0,    0,    0,    0],\n",
      "        [   1,    4,  111,  125,  341,   22,   33,  622,   69,   33, 4237, 2926,\n",
      "           19,    2,    0,    0,    0,    0],\n",
      "        [   1,  105,   62,  125,  215,   39,    4,  771,   39,    4,  108,   19,\n",
      "            2,    0,    0,    0,    0,    0],\n",
      "        [   1,    4,  803,  125, 1421, 1550,  107,   33,  219,  803,  804,   19,\n",
      "            2,    0,    0,    0,    0,    0],\n",
      "        [   1,   71,  437,  263,   75,    4,  211,   29,  341,    4,  495,  141,\n",
      "            2,    0,    0,    0,    0,    0],\n",
      "        [   1,    4,  543,  392,   22,    4,  557, 1484,    7,   58, 3383,   19,\n",
      "            2,    0,    0,    0,    0,    0],\n",
      "        [   1,    4,  363,  346,    4,  622,  623, 2684,   22,   71, 2788,   19,\n",
      "            2,    0,    0,    0,    0,    0],\n",
      "        [   1,    4,  244,   89,  164,   39,    4, 1593, 3339,  341, 4237, 2926,\n",
      "            2,    0,    0,    0,    0,    0],\n",
      "        [   1,   33,  163,  125,  215,  350,  155,    4,  324, 1789,   19,    2,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   1,    4,  163,   62,  125,  215,   52, 1764,  107,    4,  349,    2,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   1,    4, 1942,  163,   75,    4, 3657,   22,    4, 1899, 1203,    2,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   1,    4,  163, 3326,    4,  714,  170,    4,  381,   19,    2,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   1,   48,  333,   50,  313,   39,    4,  324,  677,   19,    2,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   1,    4,  356,    3,   22,    4,  741,   39,    4,  314,    2,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   1,  139,  125,    4,   11,   22,  578,    7,   82,   43,    2,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   1,   33, 1803,  534,   14,    4,    8,    3,  791,   19,    2,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   1,  331,  667,   52,   33,  427, 3046,  493, 1503,   19,    2,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   1,   48,  437,    3, 3670,  420,    4,  860,  339,   19,    2,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   1,  820,    7, 2221, 2803,  585,   39,   33,   40,    2,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   1,    4,    3,  841,   39,  896,  107,   46, 1644,    2,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
